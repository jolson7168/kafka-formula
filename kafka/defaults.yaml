{% load_yaml as rawmap %}
Ubuntu:
  user: kafka
  service: kafka-broker
  prefix: /usr/local  
  version: '0.8.2.2'
  scala_version: '2.10'
  
  # These are used to build the resources to download and isntall things.
  # Unless kafka changes the format or this mirror is no bueno, dont touch!
  name_template: 'kafka_%(scala_version)s-%(version)s'
  mirror_template: 'http://apache.mirrors.tds.net/kafka/%(version)s/%(name)s.tgz'

  config:
    port: 9092
    
    broker:
      id: 42
      id.generation.enable: true
      
    compression:
      # can also be gzip|snappy|lz4
      # producer will retain the codec set by the producer
      type: producer

    delete:
      topic.enable: False
      
    advertised:
      host.name: ~
      listeners: ~
      port: ~

    leader:
      imbalance.check.interval.seconds: 300
      imbalance.per.broker.percentage: 10
      
    auto:
      create.topics.enable: True
      leader.rebalance.enable: True
      
    log:
      dir: /tmp/kafka/logs
      dirs:
        - /tmp/kafka/data
        
      # num of messages to accept before forcing a flush
      flush.interval.messages: 1000

      # max time before flushing to disk
      flush.interval.ms: 10000

      flush.offset.checkpoint.interval.ms: 60000
      flush.scheduler.interval.ms: 9223372036854775807
      
      retention.bytes: -1
      # check file eligibility for deletion
      retention.check.interval.ms: 300000
      
      # minimum age required for deletion
      retention.hours: 168
      # if set, overrides hours
      retention.minutes: ~
      # if set, overrides hours & minutes
      retention.ms: ~

      roll.hours: 160
      roll.jitter.hours: 0
      roll.jitter.ms: ~

      # max size of a log segment files 
      segment.bytes: 1073741824
      segment.delete.delay.ms: 60000
      
      

      cleaner.backoff.ms: 15000
      cleaner.dedupe.buffer.size: 134217728
      cleaner.delete.retention.ms: 86400000
      cleaner.enable: true
      cleaner.io.buffer.load.factor: 0.9
      # total memory used for log cleaning across all cleaner threads
      cleaner.io.buffer.size: 524288
      # Cleanter will be throttled so that the sum of read+write i/o is less than this
      cleaner.io.max.bytes.per.second: ~
      # min ratio of dirty/total logs to trigger a clean
      cleaner.min.cleanable.ratio: 0.5
      cleaner.threads: 1
      
      # can be delete or compact
      cleanup.policy: delete
      
      index.interval.bytes: 4096
      index.size.max.bytes: 10485760

      # pre allocate a file before using new segment, set to true on winblows
      preallocate: false

      

    # Must be format: [type, host, port]
    # example:  `ssl://0.0.0.0:9093` or `plaintext://server.mydomain.com:9092`
    listeners:
      - 'plaintext://0.0.0.0:9092'
      
    zookeeper:
      chroot: '/kafka'
      connect:
        - 'localhost:2181'
      connection.timeout.ms: ~
      session.timeout.ms: 6000
      set.acl: false

      
    principal:
      # namespace of the class to use, override this if you really know your stuff
      builder.class: ~
      
    num:
      network.threads: 3
      io.threads: 8

      # default number of partitions
      partitions: 1
      replica.fetchers: 1
      recovery.threads.per.data.dir: 1
      
      
    socket:
      send.buffer.bytes: 102400
      receive.buffer.bytes: 102400
      request.max.bytes: 104857600

    min:
      insync.replicas: 1

    offset:
      metadata.max_bytes: 4096
      commit.required.acks: -1
      commit.timeout.ms: 5000
      load.buffer.size: 5242880
      retention.check.interval.ms: 600000
      retention.minutes: 1440
      topic.compression.codec: 0
      topic.num.partitions: 50
      topic.replication.factor: 3
      topic.segment.bytes: 104857600

    queued:
      max.requests: 500

    quota:
      consumer.default: ~
      producer.default: ~

    replica:
      fetch.max.bytes: 1048576
      fetch.min.bytes: 1
      fetch.wait.max.ms: 500
      high.watermark.checkpoint.interval.ms: 5000
      lag.time.max.ms: 10000
      socket.receive.buffer.bytes: 65536
      socket.timeout.ms: 30000
      fetch.backoff.ms: 1000
      
    request:
      timeout.ms: 30000

    unclean:
      leader.election.enable: true

    connections:
      max.idle.ms: 600000

    controlled:
      shutdown.enable: true
      shutdown.max.retries: 3
      shutdown.retry.backoff.ms: 5000

    controller:
      socket.timeout.ms: 30000

    default:
      replication.factor: 1

    fetch:
      purgatory.purge.interval.requests: 1000

    group:
      max.session.timeout.ms: 30000
      min.session.timeout.ms: 6000

    inter:
      broker.protocol.version: ~

    max:
      connections.per.ip: 2147483647
      connections.per.io.overrides: ""

    producer:
      purgatory.purge.interval.requests: 1000

      
{% endload %}
